OpenMCL 1.1-pre-051027

- A lot of internal changes in the way that special bindings, UNWIND-PROTECT,
  and WITHOUT-INTERRUPTS are implemented (and in how they interact with
  each other.

  One user-visible aspect of this is that UNWIND-PROTECT cleanup forms
  are run with interrupts disabled (the protected form is run with
  interrupts enabled if they were enabled on entry to the UNWIND-PROTECT.)
  This means that something like:

  (unwind-protect
      nil
    (loop))

  will loop uninterruptibly.

- CCL:WITH-INTERRUPTS-ENABLED &body body executes the body with interrupts
  enabled.  The example above could be rewritten as:

  (unwind-protect
      nil
    (with-interrupts-enabled (loop)))

  and the loop would be interruptible.

  These changes introduce binary incompatibility (the FASL version changed,
  as did an internal version number that tries to keep the kernel and
  heap image in synch.)

  Things basically work, but there may be lingering bugs (e.g., as of
  a little while ago, QUIT didn't work because the initial process
  was running with interrupts disabled.)

- PROCESS-TERMINATION-SEMAPHORE
  MAKE-PROCESS and PROCESS-RUN-FUNCTION accept a :TERMINATION-SEMAPHORE
  argument; processes have a PROCESS-TERMINATION-SEMAPHORE accessor
  method.  If the argument is specified and non-null, its value should
  of type SEMAPHORE.

  If a process dies by any means after it's been successfully enabled
  and it has a non-null termination semaphore "at the time of its death", 
  that semaphore will be signaled just before the underlying OS thread
  is destroyed.

  SETF can be used with PROCESS-TERMINATION-SEMAPHORE to change or
  clear a the termination semaphore of a process.  If the target
  process is not the current process when this happens, it's possible
  that the process could die before the SETF takes effect; this
  possibility must be addressed at the application level (i.e., the
  implementation doesn't try to synchronize the calling thread and
  the target in any way.

  A simple example:

  (let* ((s (make-semaphore)))
    (process-run-function `(:name "sleepy" :termination-semaphore ,s)
                           #'(lambda () (sleep 10)))
    (wait-on-semaphore s))

  The calling thread will wait for (roughly) 10 seconds (until the
  "sleepy" thread has had its nap and signals its termination semaphore.)

- A change that was introduced prior to 0.14.3 led to strange, usually
  fatal crashes (usually an unhandled bus error, occasionally a cryptic
  "can't find active area" message and a trip to the kernel debugger)
  under Darwin.  This was caused by an attempt to use certain Mach
  primitives to suspend and resume threads (the way that those
  primitives were used, Mach exception messages were sometimes sent
  twice if the first send was interrupted, and the second send occurred
  after the exception had already been handled (because the first send
  was recieved but not replied to ...)

  1.0 backed out of this change, and used signal handling primitives
  (instead of Mach primitives) to suspend and resume threads.  I -think-
  that I understand the issue with the Mach primitives 
  (#_thread_abort_safely isn't necessary and caused the duplicate
  exception messages to be sent) and have tried to revert to using
  the Mach thread suspension mechanisms.  (If unhandled bus errors -
  that exit to the shell - or cryptic "can't find active area" messages
  reappear, this experiment will be shown to be a failure.)

  There are some obscure but good reasons for favoring the Mach
  primiitves, so it'd be good to know if the problem with using them
  has indeed been identified.

  (The test case involves bad luck and bad timing: two or more
  threads having pending exceptions at the same time and the thread
  whose exception is handled first tries to suspend the others, typically
  on behalf of the GC.  It was possible to run stress tests for many
  hours in 0.14.3 without encountering the bug, and possible to
  encounter it under seemingly light loads.)

- INCF and DECF argument order and fixnum arithmetic.

  Bryan fixed some ANSI test failures related to the order in which INCF
  and DECF evaluate their args.  (One example is:

  (let* ((x 3))
    (incf x (setq x 5)))

  where the correct answer is 10, not 8.)  We both found that fixing
  some cases involving INCF caused some OpenMCL code to compile
  incorrectly and were nervous about introducing these changes fairly
  late in the development cycle, so we backed out of them prior to
  the 1.0 code freeze.

  The reasons for the miscompiled code have to do with how the
  compiler interprets fixnum declarations under typical optimization
  settings.  If A and B are both declared to be FIXNUMS, then
  the expression

  (setq a (+ a b))

  will usually compile to a simple ADD instruction (with no overflow
  checking); if A and B are fixnums, the result will be a fixnum,
  though if an undetected overflow occurred in the addition, the
  result might be missing a significant bit.

  There was code in OpenMCL that assumed that

  (incf a b)

  was exactly the same as

  (setq a (+ a b))

  and in fact that was true under the old (incorrect) definition of
  INCF.  The new definition introduced some temporary bindings:

  (let* ((...)
         (#:temp (+ a b))
         (...))
     (setq a #:temp))

  In this case, the addition was allowed to generate an overflow
  (no type declaration on #:temp), and the SETQ quietly violated
  a type declaration (assigning a non-FIXNUM value to A), leading
  to further problems.

  So far, I found a couple of cases of this in the OpenMCL sources.
  (FWIW, both functions were originally transliterated from C code
  and were trying to mimic C's silent overflow behavior.)

  Moral: if you have code that assumes that INCF or DECF expand
  into simple assignments and are trying to exploit the ways that
  those assignments interact with type declarations, you may
  want to review those assumptions.  If you write code that has
  side effects in the DELTA arguments of INCF or DECF rorms,
  you'll (hopefully) be pleased to see that Bryan's changes 
  allow these side-effects to be handled correctly (at the
  right time.)  If you don't fall into either of these categories,
  you probably won't notice any difference ...

- 64-bit Linux support

  There's a 64-bit LinuxPPC heap image and some rudimentary (libc-only)
  64-bit Linux interfaces in the testing directory.

  (Unlike 64-bit Darwin, 64-bit Linux distributions typically provide
  64-bit versions of "all" standard libraries; I haven't gotten around
  to building 64-bit gnome/gtk/X11/... interfaces yet, but wouldn't
  expect there to be a problem.)

  The 64-bit Linux OpenMCL seems to basically work, but ... OpenMCL
  likes to map its kernel into low addresses (around #x5000); this
  allows compiled lisp code to use conditional branches to "short"
  (16-bit) absolute addresses.  Newer Linux kernels provide a
  "vdso" shared library that's intended to simply communication
  between the OS kernel and userspace libraries and programs; when
  a program is mapped at "non-standard" addresses, the vdso gets
  mapped at address 0.

  I don't fully understand the imlications of this (beyond the fact that
  indirecting through a NULL pointer will access bits and pieces
  of the vdso instead of segfaulting.)  As far as I know, this is
  seen as a minor bug in the Linux kernel, and I -think- that I've
  seen kernel ChangeLog entries that indicate that the problem's been
  fixed in the relatively recent past (and will likely start to
  make it into Linux distributions in the near future.)

  That said - and seeing a library at address 0 certainly makes me a
  little nervous - the LinuxPPC64 port seems to work at least as
  well as the DarwinPPC64 port does (i.e., there may be word-size
  or other bugs lurking around or hiding in plain sight, but it's
  not usually easy to encounter them.)

- As documented (and as hasn't been true in a long time), EOF
  from *STANDARD-INPUT* terminates the REPL when the --batch argument
  is in effect (even if *STANDARD-INPUT* is a tty.)

- QUIT does a FRESH-LINE on and FORCE-OUTPUT to the standard output
  stream (people had reported that output wasn't always flushed
  when --batch or --eval was used; 1.0 was better about this than
  previous versions were, but it still wasn't reliable.)
